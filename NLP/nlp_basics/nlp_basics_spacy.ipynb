{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# NLP Basics with spacy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm"
   ]
  },
  {
   "source": [
    "## Spacy basics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This nlp is the pipeline object with which we use different techniques\n",
    "# This is loading a model, and we loaded an nlp english model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will parse this in different tokens\n",
    "doc = nlp(u\"Tesla is looking at buying U.S. startup for $6 million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Tesla is looking at buying U.S. startup for $6 million"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tesla PROPN nsubj\nis AUX aux\nlooking VERB ROOT\nat ADP prep\nbuying VERB pcomp\nU.S. PROPN compound\nstartup NOUN dobj\nfor ADP prep\n$ SYM quantmod\n6 NUM compound\nmillion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "# token.pos_ -> provides the type of english character it is\n",
    "# dep_ -> syntactic dependency\n",
    "for token in doc:\n",
    "    print(token,token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x28c7feccf70>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x28c7ee9c2e0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x28c7ee9c160>)]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tesla\n",
      "be\n",
      "look\n",
      "at\n",
      "buy\n",
      "U.S.\n",
      "startup\n",
      "for\n",
      "$\n",
      "6\n",
      "million\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"Leverage agile frameworks to provide a robust synopsis for high leveloverviews. Iterative approaches to corporate strategy foster collaborativethinking to further the overall value proposition. Organically grow theholistic world view of disruptive innovation via workplace diversity andempowerment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = doc2[16:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "strategy foster collaborativethinking to further the overall value proposition. Organically grow theholistic world\n"
     ]
    }
   ],
   "source": [
    "print(quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "type(quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Leverage agile frameworks to provide a robust synopsis for high leveloverviews.\nIterative approaches to corporate strategy foster collaborativethinking to further the overall value proposition.\nOrganically grow theholistic world view of disruptive innovation via workplace diversity andempowerment.\n"
     ]
    }
   ],
   "source": [
    "# Getting different sentences\n",
    "for sentence in doc2.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "531"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# checking the vocab of the language loaded \n",
    "len(doc.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document objects does not support reassignment like doc[2] = \"something\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting named entity\n",
    "doc3 = nlp(u\"Apple to build a factory in Hong Kong for $6 million.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apple ORG\nHong Kong GPE\n$6 million MONEY\n"
     ]
    }
   ],
   "source": [
    "for entity in doc3.ents:\n",
    "    print(entity, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Apple\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n to build a factory in \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Hong Kong\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n for \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    $6 million\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n</mark>\n.</div></span>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "displacy.render(doc3, style='ent', jupyter=True)"
   ]
  },
  {
   "source": [
    "# Stemming\n",
    "### porter's algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"run\", \"runner\", \"ran\", \"runs\",\"easily\", \"fairly\",\"fairness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "run -----> run\nrunner -----> runner\nran -----> ran\nruns -----> run\neasily -----> easili\nfairly -----> fairli\nfairness -----> fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"{word} -----> {p_stemmer.stem(word=word)}\")"
   ]
  },
  {
   "source": [
    "### Snowball stemmer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "run -----> run\nrunner -----> runner\nran -----> ran\nruns -----> run\neasily -----> easili\nfairly -----> fair\nfairness -----> fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"{word} -----> {snowball_stemmer.stem(word=word)}\")"
   ]
  },
  {
   "source": [
    "# Lemmatization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = nlp(u\"I am a runner running in the race because I love to run since i ran today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I \t PRON \t -PRON-\nam \t AUX \t be\na \t DET \t a\nrunner \t NOUN \t runner\nrunning \t VERB \t run\nin \t ADP \t in\nthe \t DET \t the\nrace \t NOUN \t race\nbecause \t SCONJ \t because\nI \t PRON \t -PRON-\nlove \t VERB \t love\nto \t PART \t to\nrun \t VERB \t run\nsince \t SCONJ \t since\ni \t PRON \t i\nran \t VERB \t run\ntoday \t NOUN \t today\n"
     ]
    }
   ],
   "source": [
    "for token in doc_l:\n",
    "    print(token.text, \"\\t\", token.pos_, \"\\t\", token.lemma_)"
   ]
  },
  {
   "source": [
    "# Stop words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'should', 'there', 'together', 'has', 'make', 'same', 'us', 'somehow', 'yourselves', 'yet', 'your', 'who', 'formerly', 'eleven', 'due', 'some', 'whom', 'with', 'then', 'hereafter', 'regarding', 'between', 'else', 'indeed', 'those', 'cannot', 'whereby', 'namely', 'five', 'besides', 'off', 'which', 'its', 'one', 'it', 'least', 'before', 'please', 'what', 'call', 'mostly', 'seeming', 'until', 'across', 'my', '‘m', 'thus', 'a', 'whoever', 'seems', 'although', 'latterly', 'herein', 'less', 'none', 'something', 'thereupon', 'could', 'beside', 'via', 'here', 'well', 'amongst', 'without', 'three', 'last', 'most', 'yours', 'six', 'into', 'used', 're', 'whereafter', 'various', '‘s', 'was', 'might', 'amount', 'sometime', 'two', 'see', 'among', 'during', 'nevertheless', 'through', 'am', 'few', 'behind', 'anything', '’re', 'per', 'back', 'still', 'but', 'once', '’ll', 'get', 'anyway', 'wherein', 'up', 'thru', 'hundred', 'under', 'ca', 'when', 'while', 'whereas', 'itself', 'quite', 'she', 'is', 'former', 'always', 'too', 'myself', 'been', 'that', \"'s\", 'becomes', '’d', 'enough', 'first', 'elsewhere', '‘re', 'he', 'do', 'towards', 'over', 'whither', 'unless', 'another', 'somewhere', 'twenty', \"n't\", 'give', 'hereby', 'top', 'full', 'much', 'therein', '‘d', 'can', 'really', 'on', \"'re\", 'meanwhile', 'say', 'everything', 'nobody', 'nothing', '’s', 'sixty', 'ten', '’m', 'go', 'are', 'move', 'within', 'show', 'alone', 'noone', 'serious', 'for', 'did', 'many', 'keep', 'already', 'how', 'herself', 'their', 'in', 'done', 'as', 'each', 'does', 'ours', \"'m\", 'around', 'and', 'no', 'also', 'why', 'take', 'made', 'an', 'perhaps', 'they', 'nowhere', 'we', 'down', 'since', \"'d\", 'third', 'though', 'any', 'to', 'fifteen', \"'ve\", 'sometimes', 'must', '‘ll', 'you', 'further', 'himself', 'whether', 'n‘t', 'eight', 'nine', 'both', 'or', 'hence', '’ve', 'so', 'onto', 'wherever', 'throughout', 'only', 'again', 'ourselves', 'everyone', 'will', 'would', 'every', 'this', 'often', 'than', 'front', 'moreover', 'name', 'yourself', 'beyond', 'forty', 'at', 'someone', 'from', 'next', 'him', 'becoming', 'have', 'seem', 'afterwards', 'may', 'such', 'neither', 'became', 'nor', 'thence', 'our', 'whenever', 'ever', 'bottom', 'even', 'me', 'thereafter', 'thereby', 'themselves', 'of', 'put', 'whence', 'seemed', 'doing', 'either', 'by', 'empty', '‘ve', 'just', 'these', 'were', 'anyone', 'fifty', 'whose', 'very', 'below', 'where', 'his', \"'ll\", 'others', 'therefore', 'more', 'them', 'using', 'after', 'own', 'however', 'twelve', 'being', 'i', 'whole', 'anywhere', 'side', 'if', 'above', 'mine', 'against', 'latter', 'about', 'not', 'hers', 'because', 'almost', 'be', 'the', 'four', 'all', 'become', 'hereupon', 'now', 'several', 'along', 'anyhow', 'had', 'whatever', 'whereupon', 'never', 'upon', 'part', 'beforehand', 'her', 'other', 'rather', 'everywhere', 'toward', 'out', 'except', 'otherwise', 'n’t'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a custom stop word\n",
    "nlp.Defaults.stop_words.add(\"btw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "nlp.vocab[\"btw\"].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing a stopword\n",
    "nlp.Defaults.stop_words.remove(\"already\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "source": [
    "## Phrase Matching and Vocabulary\n",
    "So far we've seen how a body of text is divided into tokens, and how individual tokens are parsed and tagged with parts of speech, dependencies and lemmas.\n",
    "\n",
    "In this section we will identify and label specific phrases that match patterns we can define ourselves. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = [{'LOWER': 'solarpower'}]\n",
    "pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\n",
    "pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\n",
    "\n",
    "matcher.add('SolarPower', None, pattern1, pattern2, pattern3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'The Solar Power industry continues to grow as demand \\\n",
    "for solarpower increases. Solar-power cars are gaining popularity.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
     ]
    }
   ],
   "source": [
    "found_matches = matcher(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "source": [
    "This found both two-word patterns, with and without the hyphen!\n",
    "\n",
    "The following quantifiers can be passed to the `'OP'` key:\n",
    "<table><tr><th>OP</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
    "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
    "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
    "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
    "</table>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Assesment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..\\sample_files\\owlcreek.txt\") as f:\n",
    "    owl_doc = nlp(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AN OCCURRENCE AT OWL CREEK BRIDGE\n\nby Ambrose Bierce\n\nI\n\nA man stood upon a railroad bridge in northern Alabama, looking down\ninto the swift water twenty feet below.  \n"
     ]
    }
   ],
   "source": [
    "print(owl_doc[:36])"
   ]
  },
  {
   "source": [
    "### <b>How many tokens are in the file</b>\n",
    "### How many sentences are in the file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4835\n"
     ]
    }
   ],
   "source": [
    "print(len(owl_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "sent_list3 = []\n",
    "for senten in owl_doc.sents:\n",
    "    sent_list.append(senten)\n",
    "print(len(sent_list3))"
   ]
  },
  {
   "source": [
    "# Named Entity Recognition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ents(doc):\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            print(f\"{ent.text} - {ent.label_} - {str(spacy.explain(ent.label_))}\")\n",
    "    else:\n",
    "        print(\"No Entities found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "today - DATE - Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "show_ents(doc_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apple - ORG - Companies, agencies, institutions, etc.\nHong Kong - GPE - Countries, cities, states\n$6 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "show_ents(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ent = nlp(\"May I go to the washington DC. next may to see my friend josh and see the washington monument\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "washington DC - GPE - Countries, cities, states\nnext may - DATE - Absolute or relative dates or periods\njosh - PERSON - People, including fictional\nwashington - GPE - Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "show_ents(doc_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Apple to build a factory in Hong Kong for $6 million."
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "brown fox\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_en(\"The quick brown fox jumps over the lazy dog\") \n",
    "print(doc[2:4])"
   ]
  }
 ]
}